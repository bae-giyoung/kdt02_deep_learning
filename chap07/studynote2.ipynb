{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31077fa6",
   "metadata": {},
   "source": [
    "# LSTM을 이용한 금 선물 가격 예측\n",
    "- svm => \"반복횟수가 모자라요!\"\n",
    "- 24GB[(X/32) * 1000]\n",
    "- 시퀸스 길이: 30일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ffd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = ['Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c348d",
   "metadata": {},
   "source": [
    "### 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da6c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, index_col=\"Date\", parse_dates=True)\n",
    "    return df\n",
    "\n",
    "def split_data(df, train_ratio=0.8, val_ratio=0.1):\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    val_size = int(len(df) * val_ratio)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    val_df = df.iloc[train_size : train_size + val_size]\n",
    "    test_df = df.iloc[train_size + val_size :]\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e979327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaler(train_df):\n",
    "    scaler = {}\n",
    "    for col in train_df.columns:\n",
    "        scaler[col] = {\"min\": train_df[col].min(), \"max\": train_df[col].max()}\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e628819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, scaler):\n",
    "    scaled_data = data.copy()\n",
    "    for col in data.columns:\n",
    "        min_val = scaler[col][\"min\"]\n",
    "        max_val = scaler[col][\"max\"]\n",
    "        scaled_data[col] = (data[col] - min_val) / (max_val - min_val)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c55d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivariate_sequences(data, target_col, seq_length):\n",
    "    xs, ys = [], []\n",
    "    data_np = data.values\n",
    "    target_idx = data.columns.get_loc(target_col)\n",
    "\n",
    "    for i in range(len(data_np) - seq_length):\n",
    "        x = data_np[i : i + seq_length]\n",
    "        y = data_np[i + seq_length, target_idx]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845be896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(X_train, y_train, X_val, y_val, batch_size):\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    X_val = torch.from_numpy(X_val).float()\n",
    "    y_val = torch.from_numpy(y_val).float()\n",
    "\n",
    "    # TODO1\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e5a5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    # TODO2\n",
    "    def __init__(self, input_size=3, hidden_size=50, num_layers=2, output_size=1):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, # (batch, seq, feature) 반드시 이 순서로 되게 True로 설정한다\n",
    "                            dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x) # output, (h_n, c_n) # (N, L, D * H_{out})\n",
    "        out = self.fc(out[:, -1, :]) # \n",
    "        # batch_size, seq_len, hidden_size\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fed5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, path=\"checkpoint.pt\"):\n",
    "        self.patience, self.verbose, self.delta, self.path = (\n",
    "            patience,\n",
    "            verbose,\n",
    "            delta,\n",
    "            path,\n",
    "        )\n",
    "        self.counter, self.best_score, self.early_stop, self.val_loss_min = (\n",
    "            0,\n",
    "            None,\n",
    "            False,\n",
    "            np.inf,\n",
    "        )\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...\"\n",
    "            )\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e26602",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1538069226.py, line 15)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfor batch\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, device, num_epochs, learning_rate, patience=10\n",
    "):\n",
    "  # TODO3\n",
    "  # 손실함수\n",
    "  criterion = nn.MSELoss()\n",
    "  # 최적화함수\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  # ES\n",
    "  early_stopping = EarlyStopping(patience=patience, verbos=True) # verbos는 학습할때만 True로 실제 서비스시엔 False\n",
    "\n",
    "  # 반복문(num_epochs)\n",
    "  for each in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "      batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "      output = model(batch_X)\n",
    "      loss = criterion(optimizer, batch_y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    model.eval() # 모델을 얼린다\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "      for batch_X, batch_y in val_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        output = model(batch_X)\n",
    "        val_losses.append(criterion(output, batch_y).item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch [{each+1} / {num_epochs}]\")\n",
    "\n",
    "    early_stop = early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stop.ear:\n",
    "      print(\"Early Stop\")\n",
    "      break # 매우 중요!!!!!\n",
    "\n",
    "  # 학습모드 => train_loader\n",
    "  # 평가모드 => val_loader\n",
    "\n",
    "  # 모델을 반환\n",
    "  model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize(\n",
    "    model, X_test, y_test, test_df, scaler, target_column, seq_length, device\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.to(device)).cpu().numpy()\n",
    "\n",
    "    predicted_prices = inverse_scale_data(test_outputs, scaler, target_column)\n",
    "    actual_prices = inverse_scale_data(y_test.numpy(), scaler, target_column)\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.title(f\"Futures Gold Price Prediction (LSTM)\")\n",
    "    plt.plot(\n",
    "        test_df.index[seq_length:], actual_prices, label=\"Actual Price\", color=\"blue\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        test_df.index[seq_length:],\n",
    "        predicted_prices,\n",
    "        label=\"Predicted Price\",\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return predicted_prices, actual_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale_data(data, scaler, target_col):\n",
    "    min_val = scaler[target_col][\"min\"]\n",
    "    max_val = scaler[target_col][\"max\"]\n",
    "    return data * (max_val - min_val) + min_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3771f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/stock_bond_futures_data.csv\"\n",
    "target_col = \"Futures_Gold\"\n",
    "seq_length = 30 # LSTM에서는 하이퍼 파라미터다. 아주 중요하다! 주식의 1주일 - 5일, 선물의 1주일 - 7일\n",
    "batch_size = 32 # 보통 8부터 시작해서 리소스 사용량 간보고 수치 잡는다.\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d327d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(file_path)\n",
    "train_df, val_df, test_df = split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f535c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = create_scaler(train_df)\n",
    "trained_scaled = scale_data(train_df, scaler)\n",
    "val_scaled = scale_data(val_df, scaler)\n",
    "test_scaled = scale_data(test_df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22784920",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_multivariate_sequences(trained_scaled, target_col, seq_length)\n",
    "X_val, y_val = create_multivariate_sequences(val_scaled, target_col, seq_length)\n",
    "X_test, y_test = create_multivariate_sequences(test_scaled, target_col, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_data_loaders(X_train, y_train, X_val, y_val, batch_size)\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b038c2",
   "metadata": {},
   "source": [
    "#### ===== 데이터 가져오기 끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceModel(input_size=X_train.shape[2]).to(device)\n",
    "train_model = train_model(model, train_loader, val_loader, device, num_epochs, learning_rate, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4609, 30, 3) (549, 30, 3) (551, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "# (4609, 30, 3)\n",
    "# 4609 => \n",
    "# 30 => seq_length\n",
    "# 3 => feature 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf9a65d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4609, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdd5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# TODO1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a300d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[50,11,32]\n",
    "# out[:,-1,:]\n",
    "# [50,11,32]\n",
    "\n",
    "# out[:,-1,:] (32,30,50) -1은 맨 마지막\n",
    "# (32,30,50) => (32, 50)\n",
    "# - 배치 (4609 데이터를 사용해서)\n",
    "# - 시퀸스 (30개의 데이터를 학습)\n",
    "# - 은닉층 (50개의 메모리를 활용)\n",
    "# (32,30,50) => self.fc((32, 50))\n",
    "# => 30일까지 학습하고 1개를 예측\n",
    "\n",
    "# 1. LSTM 할 때, 반드시 시점 이후 뭘 보고 싶은건지 꼭 먼저 정해야 함\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0610a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgy_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
