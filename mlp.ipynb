{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da199640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting gradio\n",
      "  Using cached gradio-5.45.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (4.10.0)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading Brotli-1.1.0-cp312-cp312-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.6.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.13.0 (from gradio)\n",
      "  Using cached gradio_client-1.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.3-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from gradio) (11.1.0)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Using cached ruff-0.13.0-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Using cached safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Using cached starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Using cached typer-0.17.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.13.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\bgy_3_12\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Using cached transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached gradio-5.45.0-py3-none-any.whl (60.4 MB)\n",
      "Using cached gradio_client-1.13.0-py3-none-any.whl (325 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.3-cp312-cp312-win_amd64.whl (131 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached typer-0.17.4-py3-none-any.whl (46 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading Brotli-1.1.0-cp312-cp312-win_amd64.whl (357 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached ruff-0.13.0-py3-none-win_amd64.whl (13.3 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached ffmpy-0.6.1-py3-none-any.whl (5.5 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, brotli, websockets, typing-inspection, tomlkit, shellingham, semantic-version, safetensors, ruff, python-multipart, pydantic-core, orjson, mdurl, groovy, fsspec, ffmpy, annotated-types, aiofiles, uvicorn, starlette, pydantic, markdown-it-py, huggingface-hub, tokenizers, safehttpx, rich, gradio-client, fastapi, typer, transformers, gradio\n",
      "\n",
      "   -- -------------------------------------  2/31 [websockets]\n",
      "   ----- ----------------------------------  4/31 [tomlkit]\n",
      "   ---------- -----------------------------  8/31 [ruff]\n",
      "   ------------ --------------------------- 10/31 [pydantic-core]\n",
      "   ------------------ --------------------- 14/31 [fsspec]\n",
      "   -------------------- ------------------- 16/31 [annotated-types]\n",
      "   ------------------------ --------------- 19/31 [starlette]\n",
      "   ------------------------- -------------- 20/31 [pydantic]\n",
      "   ------------------------- -------------- 20/31 [pydantic]\n",
      "   ------------------------- -------------- 20/31 [pydantic]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   ---------------------------- ----------- 22/31 [huggingface-hub]\n",
      "   ---------------------------- ----------- 22/31 [huggingface-hub]\n",
      "   ---------------------------- ----------- 22/31 [huggingface-hub]\n",
      "   ----------------------------- ---------- 23/31 [tokenizers]\n",
      "   -------------------------------- ------- 25/31 [rich]\n",
      "   -------------------------------- ------- 25/31 [rich]\n",
      "   ---------------------------------- ----- 27/31 [fastapi]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [transformers]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   -------------------------------------- - 30/31 [gradio]\n",
      "   ---------------------------------------- 31/31 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-types-0.7.0 brotli-1.1.0 fastapi-0.116.1 ffmpy-0.6.1 fsspec-2025.9.0 gradio-5.45.0 gradio-client-1.13.0 groovy-0.1.2 huggingface-hub-0.34.4 markdown-it-py-4.0.0 mdurl-0.1.2 orjson-3.11.3 pydantic-2.11.7 pydantic-core-2.33.2 pydub-0.25.1 python-multipart-0.0.20 rich-14.1.0 ruff-0.13.0 safehttpx-0.1.6 safetensors-0.6.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.47.3 tokenizers-0.22.0 tomlkit-0.13.3 transformers-4.56.1 typer-0.17.4 typing-inspection-0.4.1 uvicorn-0.35.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script websockets.exe is installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts hf.exe, huggingface-cli.exe and tiny-agents.exe are installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastapi.exe is installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts transformers-cli.exe and transformers.exe are installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts gradio.exe and upload_theme.exe are installed in 'c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97339078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81ded93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a19475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Naver is \\xa0working hard right now to make it easier for consumers and their partners to get their stuff.\\nOne of the big changes will be the introduction of a new \"premium\" category:\\nA \"premium\" product will be advertised on a website that will give consumers an opportunity to upgrade to \"premium\" products.\\nHere\\'s the actual wording\\nSo, here\\'s what the new \"premium\" package looks like:\\nIn the \"premium\" category, the brand is going to have a \"premium\" logo on the product, for example a \"premium\" logo in the title, or a \"premium\" logo in the description.\\nThe product and the logo will be used in the \"premium\" category for the duration of any purchase, so long as the retailer offers a discount at the time of purchase.\\nThat means that consumers will get a coupon for all their \"premium\" products.\\nIf you already own a \"premium\" product, you can get a \"premium\" discount for the duration of your purchase.\\nThere\\'s no limit to what you can get in a \"premium\" category. Just make sure you check the \"premium\" category you need first to make sure you get'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = generator(\"Naver is \", max_length=50, num_return_sequences=1)\n",
    "generated_text # 말도 안되는 소리 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa6d2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'MJ is \\xa0really good at it.\\nI had a lot to look forward to when I first started working with it on this book. I\\'d been working with it for a while and I\\'d gotten so much better at it that I knew I needed to make a change. I had to make the book a little more mature, but I was happy with it. I was going to work with a lot more people now and I hadn\\'t seen that kind of change in someone\\'s life for a long time.\\nOne of the things that I really enjoyed about it was the way it was written. It was very original and a little bit of a departure from the typical authorial style of other writers. This book was written by a couple of writers who had been with me for a long time. It was pretty much a sequel to this book.\\nThere are a lot of people who would say, \"I don\\'t want to write a book that I read. I want to write a book that I read. I want to do something that I love.\" But they\\'re not interested in writing books by the same person that they write a book about. In the end, I want to write books by a guy who I enjoy.\\nIt\\'s not like that. It\\'s'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = generator(\"MJ is \", max_length=50, num_return_sequences=1)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT: 문장임베딩 / 이해\n",
    "# GPT-X: 텍스트 생성\n",
    "# TS: 요약\n",
    "# BART: 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19988d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" # sangrimlee/bert-base-multilingual-cased-nsmc 이 한글 모델은 토크나이저가 없다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3606bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e897fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7319e-04, 9.9983e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Good!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9f8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "classfier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa328bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    result = classfier(text)\n",
    "    return result[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad7863c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bgy_3_12\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:392: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(fn=classify_text, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgy_3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
